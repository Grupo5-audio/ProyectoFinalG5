# -*- coding: utf-8 -*-
"""CNNLSTM_L2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/106S8n107iDcIEaqMvaoWxXQXreOk8MQ5

**CNN y LSTM con l2**
Beneficios de Usar L2
Prevenci√≥n de Sobreajuste: Al penalizar pesos grandes, el modelo se vuelve m√°s generalizable.
Mejora del Rendimiento: Puede mejorar la precisi√≥n en conjuntos de datos de validaci√≥n.
Conclusi√≥n
Agregar L2 a tus modelos es una excelente manera de mejorar su robustez y rendimiento. Aseg√∫rate de experimentar con el par√°metro de regularizaci√≥n para encontrar el equilibrio adecuado para tu problema espec√≠fico.
"""

import matplotlib.pyplot as plt
from keras.models import Sequential
from keras.layers import Conv1D, MaxPooling1D, LSTM, Dense, Dropout, Flatten, TimeDistributed, Reshape, BatchNormalization
from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau  # Importar ReduceLROnPlateau
from keras.optimizers import Adam
from keras.utils import to_categorical
import tensorflow as tf
from tensorflow.keras.regularizers import l2  # Importar la funci√≥n de regularizaci√≥n L2
from sklearn.metrics import accuracy_score, f1_score
import os
import joblib
import numpy as np

def grafico_perdida(history):
    plt.figure(figsize=(12, 4))

    # P√©rdida
    plt.subplot(1, 2, 1)
    plt.plot(history.history['loss'], label='P√©rdida de Entrenamiento')
    plt.plot(history.history['val_loss'], label='P√©rdida de Validaci√≥n')
    plt.title('P√©rdida durante el Entrenamiento')
    plt.xlabel('√âpocas')
    plt.ylabel('P√©rdida')
    plt.legend()

    # Precisi√≥n
    plt.subplot(1, 2, 2)
    plt.plot(history.history['accuracy'], label='Precisi√≥n de Entrenamiento')
    plt.plot(history.history['val_accuracy'], label='Precisi√≥n de Validaci√≥n')
    plt.title('Precisi√≥n durante el Entrenamiento')
    plt.xlabel('√âpocas')
    plt.ylabel('Precisi√≥n')
    plt.legend()

    plt.tight_layout()
    plt.show()

def entrenar_modelo_cnn_lstmL2(data_path="src/", models_path="models/", epochs=50, batch_size=32):
    # Cargar datos de entrenamiento y prueba
    x_train, y_train, feature_names = joblib.load(os.path.join(data_path, "train.pkl"))
    x_test, y_test, feature_names = joblib.load(os.path.join(data_path, "test.pkl"))

    # Aplanar entradas si est√°n expandidas (para audio)
    if len(x_train.shape) > 2:
        x_train = x_train.reshape(x_train.shape[0], -1)
        x_test = x_test.reshape(x_test.shape[0], -1)

    # Cargar nombres de clases (emociones)
    class_labels_path = os.path.join(data_path, "class_labels.npy")
    if os.path.exists(class_labels_path):
        class_names = np.load(class_labels_path, allow_pickle=True)
    else:
        raise FileNotFoundError("‚ùå No se encontr√≥ el archivo class_labels.npy con los nombres de las emociones.")

    # Decodificar etiquetas One-Hot a √≠ndices
    y_train_labels = np.argmax(y_train, axis=1)
    y_test_labels = np.argmax(y_test, axis=1)

    # Ajustar las dimensiones para el modelo
    x_train = np.expand_dims(x_train, axis=2)  # Aseg√∫rate que x_train tiene forma (n_samples, 364, 1)
    x_test = np.expand_dims(x_test, axis=2)    # Aseg√∫rate que x_test tiene forma (n_samples, 364, 1)

    # Verificar las formas de los datos
    print(f"x_test shape: {x_test.shape}")  # Debe ser (n_samples, 364, 1)
    print(f"y_test shape: {y_test.shape}")    # Debe ser (n_samples, num_classes)

    # Si y_test no est√° en formato one-hot, convi√©rtelo
    if len(y_test.shape) == 1:  # Si es un vector
        num_classes = np.max(y_test) + 1  # Asumiendo que las clases son 0 a num_classes-1
        y_test = to_categorical(y_test, num_classes)

    # Definir el modelo
    model = Sequential()
    model.add(Reshape((x_train.shape[1], 1), input_shape=(x_train.shape[1],)))
    model.add(Conv1D(512, kernel_size=3, activation='relu', kernel_regularizer=l2(0.00001)))  # Aumentar unidades
    model.add(BatchNormalization())
    model.add(MaxPooling1D(pool_size=5))
    model.add(Dropout(0.3))
    model.add(Conv1D(256, kernel_size=3, activation='relu', kernel_regularizer=l2(0.00001)))  # Aumentar unidades
    model.add(BatchNormalization())
    model.add(MaxPooling1D(pool_size=5))
    model.add(Dropout(0.3))
    model.add(TimeDistributed(Flatten()))
    model.add(LSTM(200, return_sequences=True, kernel_regularizer=l2(0.00001)))  # M√°s unidades LSTM
    model.add(LSTM(200, return_sequences=True, kernel_regularizer=l2(0.00001)))
    model.add(LSTM(200, return_sequences=False, kernel_regularizer=l2(0.00001)))  # M√°s unidades LSTM
    model.add(Dropout(0.5))
    model.add(Dense(y_train.shape[1], activation='softmax', kernel_regularizer=l2(0.0001)))  # Regularizaci√≥n L2
    model.summary()

    # Compilar el modelo con una tasa de aprendizaje ajustada
    optimizer = Adam(learning_rate=0.001)  # Tasa de aprendizaje m√°s baja
    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])

    # Definir Callbacks
    checkpoint = ModelCheckpoint("best_model.keras", monitor='val_accuracy', save_best_only=True)
    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.00001)

    # Entrenar el modelo
    history = model.fit(x_train, y_train,
                        batch_size=batch_size,
                        epochs=epochs,
                        validation_data=(x_test, y_test),
                        callbacks=[checkpoint, early_stopping, reduce_lr])

    # Evaluaci√≥n del modelo
    accuracy = model.evaluate(x_test, y_test)[1] * 100
    print(f"Accuracy of our model on test data: {accuracy:.2f} %")

    # Visualizaci√≥n de la p√©rdida y precisi√≥n
    grafico_perdida(history)
    
    # üíæ Guardar modelo
    model_path = os.path.join(models_path, "cnn_lstml2.keras")
    model.save(model_path)
    print(f"üì¶ Modelo CNN_LSTM guardado en: {model_path}")
    
    # Evaluate the model
    accuracy = model.evaluate(x_test, y_test)[1] * 100 # Use x_test directly
    print(f"Accuracy of our model on test data: {accuracy:.2f} %")

    # üß™ Evaluaci√≥n en test
    y_pred_probs = model.predict(x_test)
    y_pred_labels = np.argmax(y_pred_probs, axis=1)
    #y_test_labels = np.argmax(y_test, axis=1) # Already created y_test_labels earlier

    print("üìà Evaluaci√≥n final en conjunto de prueba:")
    metrics_values(y_test_labels, y_pred_labels, class_names)

    return model, x_test, feature_names # Return model, x_test, and feature_names
