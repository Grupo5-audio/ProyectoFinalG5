# -*- coding: utf-8 -*-
"""CNNLSTM_L2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/106S8n107iDcIEaqMvaoWxXQXreOk8MQ5

**CNN y LSTM con l2**
Beneficios de Usar L2
Prevención de Sobreajuste: Al penalizar pesos grandes, el modelo se vuelve más generalizable.
Mejora del Rendimiento: Puede mejorar la precisión en conjuntos de datos de validación.
Conclusión
Agregar L2 a tus modelos es una excelente manera de mejorar su robustez y rendimiento. Asegúrate de experimentar con el parámetro de regularización para encontrar el equilibrio adecuado para tu problema específico.
"""

import matplotlib.pyplot as plt
from keras.models import Sequential
from keras.layers import Conv1D, MaxPooling1D, LSTM, Dense, Dropout, Flatten, TimeDistributed, Reshape, BatchNormalization
from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau  # Importar ReduceLROnPlateau
from keras.optimizers import Adam
from keras.utils import to_categorical
import tensorflow as tf
from tensorflow.keras.regularizers import l2  # Importar la función de regularización L2
from sklearn.metrics import accuracy_score, f1_score
import os
import joblib
import numpy as np

def grafico_perdida(history):
    plt.figure(figsize=(12, 4))

    # Pérdida
    plt.subplot(1, 2, 1)
    plt.plot(history.history['loss'], label='Pérdida de Entrenamiento')
    plt.plot(history.history['val_loss'], label='Pérdida de Validación')
    plt.title('Pérdida durante el Entrenamiento')
    plt.xlabel('Épocas')
    plt.ylabel('Pérdida')
    plt.legend()

    # Precisión
    plt.subplot(1, 2, 2)
    plt.plot(history.history['accuracy'], label='Precisión de Entrenamiento')
    plt.plot(history.history['val_accuracy'], label='Precisión de Validación')
    plt.title('Precisión durante el Entrenamiento')
    plt.xlabel('Épocas')
    plt.ylabel('Precisión')
    plt.legend()

    plt.tight_layout()
    plt.show()

def entrenar_modelo_cnn_lstmL2(data_path="src/", models_path="models/", epochs=50, batch_size=32):
    # Cargar datos de entrenamiento y prueba
    x_train, y_train, feature_names = joblib.load(os.path.join(data_path, "train.pkl"))
    x_test, y_test, feature_names = joblib.load(os.path.join(data_path, "test.pkl"))

    # Aplanar entradas si están expandidas (para audio)
    if len(x_train.shape) > 2:
        x_train = x_train.reshape(x_train.shape[0], -1)
        x_test = x_test.reshape(x_test.shape[0], -1)

    # Cargar nombres de clases (emociones)
    class_labels_path = os.path.join(data_path, "class_labels.npy")
    if os.path.exists(class_labels_path):
        class_names = np.load(class_labels_path, allow_pickle=True)
    else:
        raise FileNotFoundError("❌ No se encontró el archivo class_labels.npy con los nombres de las emociones.")

    # Decodificar etiquetas One-Hot a índices
    y_train_labels = np.argmax(y_train, axis=1)
    y_test_labels = np.argmax(y_test, axis=1)

    # Ajustar las dimensiones para el modelo
    x_train = np.expand_dims(x_train, axis=2)  # Asegúrate que x_train tiene forma (n_samples, 364, 1)
    x_test = np.expand_dims(x_test, axis=2)    # Asegúrate que x_test tiene forma (n_samples, 364, 1)

    # Verificar las formas de los datos
    print(f"x_test shape: {x_test.shape}")  # Debe ser (n_samples, 364, 1)
    print(f"y_test shape: {y_test.shape}")    # Debe ser (n_samples, num_classes)

    # Si y_test no está en formato one-hot, conviértelo
    if len(y_test.shape) == 1:  # Si es un vector
        num_classes = np.max(y_test) + 1  # Asumiendo que las clases son 0 a num_classes-1
        y_test = to_categorical(y_test, num_classes)

    # Definir el modelo
    model = Sequential()
    model.add(Reshape((x_train.shape[1], 1), input_shape=(x_train.shape[1],)))
    model.add(Conv1D(512, kernel_size=3, activation='relu', kernel_regularizer=l2(0.00001)))  # Aumentar unidades
    model.add(BatchNormalization())
    model.add(MaxPooling1D(pool_size=5))
    model.add(Dropout(0.3))
    model.add(Conv1D(256, kernel_size=3, activation='relu', kernel_regularizer=l2(0.00001)))  # Aumentar unidades
    model.add(BatchNormalization())
    model.add(MaxPooling1D(pool_size=5))
    model.add(Dropout(0.3))
    model.add(TimeDistributed(Flatten()))
    model.add(LSTM(200, return_sequences=True, kernel_regularizer=l2(0.00001)))  # Más unidades LSTM
    model.add(LSTM(200, return_sequences=True, kernel_regularizer=l2(0.00001)))
    model.add(LSTM(200, return_sequences=False, kernel_regularizer=l2(0.00001)))  # Más unidades LSTM
    model.add(Dropout(0.5))
    model.add(Dense(y_train.shape[1], activation='softmax', kernel_regularizer=l2(0.0001)))  # Regularización L2
    model.summary()

    # Compilar el modelo con una tasa de aprendizaje ajustada
    optimizer = Adam(learning_rate=0.001)  # Tasa de aprendizaje más baja
    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])

    # Definir Callbacks
    checkpoint = ModelCheckpoint("best_model.keras", monitor='val_accuracy', save_best_only=True)
    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.00001)

    # Entrenar el modelo
    history = model.fit(x_train, y_train,
                        batch_size=batch_size,
                        epochs=epochs,
                        validation_data=(x_test, y_test),
                        callbacks=[checkpoint, early_stopping, reduce_lr])

    # Evaluación del modelo
    accuracy = model.evaluate(x_test, y_test)[1] * 100
    print(f"Accuracy of our model on test data: {accuracy:.2f} %")

    # Visualización de la pérdida y precisión
    grafico_perdida(history)

    return model, history  # Retornar el modelo y el historial